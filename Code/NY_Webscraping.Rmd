---
title: "Data Sci Project"
author: "Amanda McDermott"
date: "3/29/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(jsonlite)
library(rvest)
library(purrr)
```

```{r}
api <- "Wel728AGviAZjsG5bwtsLMYkjwOGzmxO"
dates <- paste(rep(2000:2018, each = 12), rep(1:12, 19), sep = "/")
dates[229:231] <- c("2019/1", "2019/2", "2019/3")
url <- paste0('https://api.nytimes.com/svc/archive/v1/', dates, '.json?api-key=', api)

ny <- fromJSON(url[150])

# trying it with purrr
l1 <- map(url[1:100], fromJSON)
Sys.sleep(5)
l2 <- map(url[51:100], fromJSON)
Sys.sleep(5)
l3 <- map(url[101:150], fromJSON)
Sys.sleep(5)
l4 <- map(url[151:200], fromJSON)
Sys.sleep(5)
l5 <- map(url[201:231], fromJSON)
ny <- append(l1, l2, l3, l4, l5)

for(i in 1:length(url)){
  print(seq(from = i, to = 10 *i))
}

e <- 'Error in open.connection(con, "rb") : HTTP error 429.'

test <- tryCatch(fromJSON(url[1:20]), error=function(e) Sys.sleep(1))
```


```{r}
# fromJSON with tryCatch
e <- simpleError('Error in open.connection(con, "rb") : HTTP error 429.')

for(i in 1:length(url)){
test <- url %>%
  map(fromJSON) %>% 
  Sys.sleep(1)
}

map(url[1:10], fromJSON, Sys.sleep(1))

for(i in )

myfunc <- function(){
  test <- map(url, fromJSON)
}
attempt <- 1
while(attempt <= 3){
  attempt <- attempt + 1
  try(
    test2 <- myfunc()
  )
}

test <- tryCatch(myfunc(fromJSON(url[1])), error = function(e) Sys.sleep(1))

test <- tryCatch(myfunc(map(url[70:80], fromJSON)), error = function(e) Sys.sleep(1))
tryCatch(myfunc(), error = function(e) Sys.sleep(1))

test[["response"]][["docs"]]
# find areas of interest
ny <- ny[["response"]][["docs"]]

unique(ny$subsectoinName)
ny <- ny %>% 
  rename()
  select(web_url, pub_date, news_desk, section_name, subsectoinName) %>% 
  filter(section_name == c("U.S.", "World", "Education", "Health"))
  filter(subsectoinName == c("Politics", "Middle East", "Africa", "Europe", "Asia Pacific", "Americas", "Economy", "Energy & Environment"))

ny <- as.data.frame(ny)


ny_url <- read_html(ny$web_url[1])

ny$text <- NA

ny$text[1] <- ny_url %>% 
  html_nodes(".efqptxt0+ .StoryBodyCompanionColumn .evys1bk0 , .evys1bk0~ .evys1bk0+ .evys1bk0") %>% 
  html_text() %>% 
  toString()
```


```{r}
test_scrape <- function(){
api <- "Wel728AGviAZjsG5bwtsLMYkjwOGzmxO"
dates <- paste(rep(2000:2019, each = 12), rep(1:12, 19), sep = "/")
url <- paste0('https://api.nytimes.com/svc/archive/v1/', dates, '.json?api-key=', api)

ny <- fromJSON(url)

# find areas of interest
ny <- ny[["response"]][["docs"]]

unique(ny$subsectoinName)
ny <- ny %>% 
  select(web_url, pub_date, news_desk, section_name, subsectoinName) %>% 
  filter(subsectoinName == c("Politics", "Middle East", "Africa", "Europe", "Asia Pacific", "Americas", "Economy", "Energy & Environment"))

ny <- as.data.frame(ny)

for (i in seq_along(ny$web_url)){
  
  ny_url <- read_html(ny$web_url[i])
  
  ny$text <- NA
  
  ny$text[i] <- ny_url %>% 
  html_nodes(".efqptxt0+ .StoryBodyCompanionColumn .evys1bk0 , .evys1bk0~ .evys1bk0+ .evys1bk0") %>% 
  html_text() %>% 
  toString()


}

}


for (i in seq_along(ny)){
  
  ny_url <- read_html(ny$web_url[i])
  
  ny$text <- NA
  
  ny$text[i] <- ny_url %>% 
  html_nodes(".efqptxt0+ .StoryBodyCompanionColumn .evys1bk0 , .evys1bk0~ .evys1bk0+ .evys1bk0") %>% 
  html_text() %>% 
  toString()


}

```

