---
title: "Data Sci Project"
author: "Amanda McDermott"
date: "3/29/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(jsonlite)
library(rvest)
library(purrr)
library(polite)
library(lubridate)
library(data.table)
```

To gain access to the New York Times archives, you must first create a developer account and create an API key. Go to https://developer.nytimes.com/get-started for more information. Create an API key under "Archive API" which allows users to get all NYT articles for a given month. Once your app is created, copy the API key and paste into the function below. 
```{r}
api <- rstudioapi::askForSecret("Api")
```


```{r}
# create vector of dates to look at, so we're looking at NYT articles from 2000 to 2018
dates <- paste(rep(2000:2018, each = 12), rep(1:12, 19), sep = "/")
# add dates in from 2018 since there isn't a complete archive of 2018 yet
dates[229:231] <- c("2019/1", "2019/2", "2019/3")
# create urls unique to the given date and api key
url <- paste0('https://api.nytimes.com/svc/archive/v1/', dates, '.json?api-key=', api)
```

```{r}
test_function <- function(x){
  # functions that will be used with map
  f <- c("fromJSON", "Sys.sleep")
  # parameters respective to their functions
  param <- list(url2[x], 5)
  # run invoke_map
  ny <- invoke_map(f, param)
}

for(i in seq_along(url)){
  test_url[i] <- test_function(i)
}

url2 <- url[106:230]
test_url2 <- vector(mode = "list", length = 124)

for(i in seq_along(url2)){
  test_url2[i] <- test_function(i)
}

url3 <- url[231]
test_url3 <- vector(mode = "list", length = 1)

for(i in seq_along(url2)){
  test_url3[i] <- test_function(i)
}

```

```{r}
# Sections to include in apis
sections_interest <- c("World", "U.S.", "World; Washington", "World; Front Page", 
                       "Washington", "Front Page; U.S.", "World; Washington", 
                       "World; Front; Washington", "U.S.; Washington",  
                       "Front Page; U.S.; Washington", "Education; U.S.", "World; Education",
                       "Technology; U.S.; Washington", 
                       "Technology; World", "Technology; Education", 
                       "Technology; Science; Health")

# Convert first group of apis to dataframes
select_element <- function(x){
  temp <- test_url[[x]][["response"]][["docs"]]
  
  temp <- temp %>% 
  select(-c(blog, multimedia, headline, keywords, byline, word_count, 
            slideshow_credits, subsection_name)) %>% 
  filter(section_name %in% sections_interest)
}

ny_api_1 <- map(1:105, select_element) %>% rbindlist()


# Convert second group
select_element2 <- function(x){
  temp <- test_url2[[x]][["response"]][["docs"]]
  
  temp <- temp %>% 
  select(-c(blog, multimedia, headline, keywords, byline, word_count)) %>% 
  filter(section_name %in% sections_interest)
}


ny_api_2 <- map(1:125, select_element2) %>% rbindlist(fill = T)

# Convert third group
select_element3 <- function(x){
  temp <- test_url3[[x]][["response"]][["docs"]]
  
  temp <- temp %>% 
  select(-c(blog, multimedia, headline, keywords, byline, word_count, 
            slideshow_credits, subsection_name)) %>% 
  filter(section_name %in% sections_interest)
}

ny_api_3 <- map(1, select_element2) %>% rbindlist()


# clean and combine all
ny_api_1 <- ny_api_1 %>% 
  select(web_url, pub_date) %>% 
  mutate(pub_date = str_extract(pub_date, "\\d{4}-\\d{2}-\\d{2}"))

ny_api_2 <- ny_api_2 %>% 
  select(web_url, pub_date) %>% 
  mutate(pub_date = str_extract(pub_date, "\\d{4}-\\d{2}-\\d{2}"))

ny_api_3 <- ny_api_3 %>% 
  select(web_url, pub_date) %>% 
  mutate(pub_date = str_extract(pub_date, "\\d{4}-\\d{2}-\\d{2}"))

ny_apis <- rbind(ny_api_1, ny_api_2, ny_api_3)

write_csv(ny_apis,"ny_apis.csv")
```


Now to make this into a workable dataframe...
```{r not needed}
# Converting to a dataframe
# there are 7904 elements in the ny list
length(ny[[1]][["response"]][["docs"]])
list_el <- c(1:7904)

# create a function to grab elements of the list and convert them to dataframes
select_element <- function(x){
  temp <- ny[[1]][["response"]][["docs"]][[x]]
  temp <- data.frame(t(sapply(temp, c)))
}

# Make ny into a df
ny_df <- map_df(list_el, select_element)

```

Great, but now I only articles relevant to my interests. I'll filter out which articles I don't want to save time when I webscrap these later.
```{r not needed}
# unique(ny_df$section_name)
sections_interest <- c("World", "U.S.", "World; Washington", "World; Front Page", 
                       "Washington", "Front Page; U.S.", "World; Books", "World; Washington", 
                       "World; Front; Washington", "U.S.; Washington",  
                       "Front Page; U.S.; Washington", "World; Health", "Health; U.S.",
                       "Education; U.S.", "World; Education", "Technology; U.S.; Washington", 
                       "Technology; World", "Front Page; Education; U.S.; Books", 
                       "Technology; Education", "Technology; Science; Education; New York and Region",
                       "Technology; Science; Health", "Technology; Front Page; Business")

ny_urls <- ny_df %>% 
  select(-c(blog, multimedia, headline, keywords, byline, word_count, X_id, 
            slideshow_credits, subsection_name)) %>% 
  filter(section_name %in% sections_interest) %>% 
  select(web_url) # take only the urls for scraping

get_urls <- function(x){
  ny_urls$web_url[[x]]
}

elements <- c(1:917)
ny_urls <- map_chr(elements, get_urls)


```

```{r}


url <- read_html(ny_urls[1])
url %>%
  html_nodes(".story-body-text") %>% 
  html_text() %>% 
  gsub("\'", "", .) %>% 
  toString()

scrape_it <- function(x){
  url <- read_html(ny_urls[x])
  
  url %>%
  html_nodes(".story-body-text") %>% 
  html_text() %>% 
  gsub("\'", "", .) %>% 
  toString()
}



for(i in seq_along(ny_texts)){
  ny_texts[i,2] <- scrape_it(i)
}



test <- scrape(session, params = unique_ny_params[1]) %>% 
  html_nodes(".story-body-text") %>% 
  html_text() %>% 
  gsub("\'", "", .) %>% 
  toString()


# set params for polite package
unique_ny_params <- str_replace_all(ny_apis$web_url, "https://www.nytimes.com", "")

# some links have a different user agent so these will be scraped separately
special_links <- (str_detect(unique_ny_params, "https:") == T)
unique_ny_params <- unique_ny_params[special_links == F]

# Create a dataframe for the text to go into
ny_texts_add <- tibble(date = c(NA), text = c(1:5000))
ny_texts2 <- tibble(date = c(NA), text = c(898:245496)) 
# function to politely scrape
ny_scrape <- function(x){
  
bow("https://www.nytimes.com", delay = 10, force = T) %>% 
  nod(path = temp[x]) %>% 
  scrape() %>% 
  html_nodes(".story-body-text") %>% 
  html_text() %>% 
  gsub("\'", "", .) %>% 
  toString()
  
}

temp <- unique_ny_params[15001:20000]

for(i in seq_along(temp)){
  ny_texts_add[i,2] <- ny_scrape(i)
}

ny_texts <- rbind(ny_texts, ny_texts_add)

write_csv(ny_texts, "ny_texts.csv")
```

webscraping only got 897:9058 of the obs, resulting in 8161 scraped articles. 
I still need to get 1:896 and 9059:245496.

```{r}
get_text <- function(session, params){
  nod(session) %>% 
    scrape(params)
}

get_text(session, unique_ny_params[1])

# functions that will be used with map
f <- c("scrape_it", "Sys.sleep")
# parameters respective to their functions
param <- list(list(1:3), 
              list(5,5,5))
# run invoke_map
test <- invoke_map(f, param)
test2 <- test %>% reduce(rbind)


sim <- tribble(
  ~f,            ~params,
  "scrape_it",   seq(1:3),
  "Sys.sleep",   list(rep(5, 3))
)


 get_cheese <- function(session, path, params){
   nod(session, path) %>%
     scrape(params)
 }

 res <- vector("list", 5)
 # iterate over first 5 pages
 for (i in seq(5)){
   res[[i]] <- get_cheese(session,
                path = "alphabetical",
                params = paste0("page=", i)) %>%
     html_nodes("h3 a") %>%
     html_text()

 }
 res
```





