---
title: "Data Sci Project"
author: "Amanda McDermott"
date: "3/29/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(jsonlite)
library(rvest)
library(purrr)
library(polite)
library(lubridate)
library(data.table)
library(doParallel)
library(foreach)
library(tictoc)
```

To gain access to the New York Times archives, you must first create a developer account and create an API key. Go to https://developer.nytimes.com/get-started for more information. Create an API key under "Archive API" which allows users to get all NYT articles for a given month. Once your app is created, copy the API key and paste into the function below. 
```{r}
# CIZfJ8en63FOMtAJakhDaQT05NSVm6NC
api <- rstudioapi::askForSecret("Api")
```


```{r}
# create vector of dates to look at, so we're looking at NYT articles from 2000 to 2018
dates <- paste(rep(2000:2018, each = 12), rep(1:12, 19), sep = "/")
# add dates in from 2018 since there isn't a complete archive of 2018 yet
dates[229:231] <- c("2019/1", "2019/2", "2019/3")
# create urls unique to the given date and api key
url <- paste0('https://api.nytimes.com/svc/archive/v1/', dates, '.json?api-key=', api)
```

```{r}
test_function <- function(x){
  # functions that will be used with map
  f <- c("fromJSON", "Sys.sleep")
  # parameters respective to their functions
  param <- list(url2[x], 5)
  # run invoke_map
  ny <- invoke_map(f, param)
}

for(i in seq_along(url)){
  test_url[i] <- test_function(i)
}

url2 <- url[106:230]
test_url2 <- vector(mode = "list", length = 124)

for(i in seq_along(url2)){
  test_url2[i] <- test_function(i)
}

url3 <- url[231]
test_url3 <- vector(mode = "list", length = 1)

for(i in seq_along(url2)){
  test_url3[i] <- test_function(i)
}

```

```{r}
# Sections to include in apis
sections_interest <- c("World", "U.S.", "World; Washington", "World; Front Page", 
                       "Washington", "Front Page; U.S.", "World; Washington", 
                       "World; Front; Washington", "U.S.; Washington",  
                       "Front Page; U.S.; Washington", "Education; U.S.", "World; Education",
                       "Technology; U.S.; Washington", 
                       "Technology; World", "Technology; Education", 
                       "Technology; Science; Health")

# Convert first group of apis to dataframes
select_element <- function(x){
  temp <- test_url[[x]][["response"]][["docs"]]
  
  temp <- temp %>% 
  select(-c(blog, multimedia, headline, keywords, byline, word_count, 
            slideshow_credits, subsection_name)) %>% 
  filter(section_name %in% sections_interest)
}

ny_api_1 <- map(1:105, select_element) %>% rbindlist()


# Convert second group
select_element2 <- function(x){
  temp <- test_url2[[x]][["response"]][["docs"]]
  
  temp <- temp %>% 
  select(-c(blog, multimedia, headline, keywords, byline, word_count)) %>% 
  filter(section_name %in% sections_interest)
}


ny_api_2 <- map(1:125, select_element2) %>% rbindlist(fill = T)

# Convert third group
select_element3 <- function(x){
  temp <- test_url3[[x]][["response"]][["docs"]]
  
  temp <- temp %>% 
  select(-c(blog, multimedia, headline, keywords, byline, word_count, 
            slideshow_credits, subsection_name)) %>% 
  filter(section_name %in% sections_interest)
}

ny_api_3 <- map(1, select_element2) %>% rbindlist()


# clean and combine all
ny_api_1 <- ny_api_1 %>% 
  select(web_url, pub_date) %>% 
  mutate(pub_date = str_extract(pub_date, "\\d{4}-\\d{2}-\\d{2}"))

ny_api_2 <- ny_api_2 %>% 
  select(web_url, pub_date) %>% 
  mutate(pub_date = str_extract(pub_date, "\\d{4}-\\d{2}-\\d{2}"))

ny_api_3 <- ny_api_3 %>% 
  select(web_url, pub_date) %>% 
  mutate(pub_date = str_extract(pub_date, "\\d{4}-\\d{2}-\\d{2}"))

ny_apis <- rbind(ny_api_1, ny_api_2, ny_api_3)

write_csv(ny_apis,"ny_apis.csv")
```


get articles
```{r}


url <- read_html(ny_urls[1])
url %>%
  html_nodes(".story-body-text") %>% 
  html_text() %>% 
  gsub("\'", "", .) %>% 
  toString()

scrape_it <- function(x){
  url <- read_html(ny_urls[x])
  
  url %>%
  html_nodes(".story-body-text") %>% 
  html_text() %>% 
  gsub("\'", "", .) %>% 
  toString()
}



for(i in seq_along(ny_texts)){
  ny_texts[i,2] <- scrape_it(i)
}



test <- scrape(session, params = unique_ny_params[1]) %>% 
  html_nodes(".story-body-text") %>% 
  html_text() %>% 
  gsub("\'", "", .) %>% 
  toString()


# set params for polite package
unique_ny_params <- str_replace_all(ny_apis$web_url, "https://www.nytimes.com", "")

# some links have a different user agent so these will be scraped separately
special_links <- (str_detect(unique_ny_params, "https:") == T)

unique_ny_params <- unique_ny_params[special_links == F]

unique_ny_params_2 <- unique_ny_params[special_links == T]



temp <- unique_ny_params[25997:27000]

# Create a dataframe for the text to go into
ny_texts_add <- vector("character", length(temp))
 
# function to politely scrape
ny_scrape <- function(x){
  
bow("https://www.nytimes.com", delay = 10, force = T) %>% 
  nod(path = temp[x]) %>% 
  scrape() %>% 
  html_nodes(".story-body-text") %>% 
  html_text() %>% 
  gsub("\'", "", .) %>% 
  toString()
  
}

tic()
for(i in seq_along(temp2)){
  ny_texts_add[i,2] <- ny_scrape(i)
}
toc()

tic()
ny_text_add <- seq_along(temp2) %>%
  partition(temp2) %>% 
  plyr::llply(ny_scrape) %>% 
  reduce(rbind) %>% 
  as_tibble()
toc()







# fastest webscraper so far
# fastest webscraper so far
tic()
ny_texts_add <- foreach(i = 1:1004, .inorder = T, .combine = 'rbind') %dopar% {
  ny_scrape(i) %>% as_tibble()
} 
toc()

ny_texts_add$date <- NA
ny_texts_add <- ny_texts_add %>% 
  rename(text = "value") %>% 
  select(date, text)

ny_texts <- rbind(ny_texts, ny_texts_add)

ny_texts_add <- NULL


write_csv(ny_texts, "ny_texts.csv")


# dates
ny_apis_dates <- ny_apis %>% 
  mutate(T_F = str_detect(web_url, "https://www.nytimes.com"))


ny_apis_dates %>% filter(T_F == TRUE) %>%  View()

link_dates <- str_c("https://www.nytimes.com", unique_ny_params)

dates <- ny_apis$pub_date[ny_apis$web_url %in% link_dates]


for_joining <- link_dates[1:26998]
ny_texts <- cbind(ny_texts, for_joining)

ny_texts <- ny_apis %>% 
  filter(web_url %in% link_dates) %>% 
  left_join(ny_texts, by = c("web_url" = "for_joining"))

  



ny_apis$pub_date[!(ny_apis$web_url %in% link_dates)]
```

webscraping only got 897:9058 of the obs, resulting in 8161 scraped articles. 
I still need to get 1:896 and 9059:245496.

```{r}
# attempt with parallel

num_cores <- detectCores() - 1
cl <- makeCluster(num_cores, type = "FORK")
registerDoParallel(cl)
```


