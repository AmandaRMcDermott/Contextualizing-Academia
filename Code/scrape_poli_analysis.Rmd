---
title: "Jour_pols_webscraping"
author: "Amanda McDermott"
date: "4/16/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(jsonlite)
library(rvest)
library(purrr)
library(lubridate)
library(topicmodels)
library(tm)
library(tidytext)
library(ldatuning)
library(beepr)
```

```{r}
# base link - this will be used to complete partial links throughout the scraping process
base_link <- "https://www.cambridge.org"

# Let's get the volume links first
vol_links <- read_html("https://www.cambridge.org/core/journals/political-analysis/all-issues")

vol_links <- vol_links %>% 
  html_nodes(".fourth > li > ul > li > a") %>%
  html_attr("href")

# combine with base_link to get the full link
full_vol_links <- map2_chr(base_link, vol_links, paste0)


# these links give us access to the text
get_html_urls <- function(x){
  url <- read_html(full_vol_links[x])
  
 url <-  url %>%  
    html_nodes(".links > li > a") %>% 
    html_attr("href") %>% 
    unlist() %>% 
    as.vector()
 
 # links with "core/product" in their string will lead us to the scrapable pages
 keep_url <- str_detect(url, "core/product")
 url <- url[keep_url == T]
 
}

html_links <- map(seq_along(full_vol_links), get_html_urls)
# combine into a character vector
html_links <- unlist(html_links)

# combine main link with each partial link
html_links <- map2_chr(base_link, html_links, paste0)


# function to scrape
scrape <- function(x){
  read_links <- read_html(html_links[x])
  
  read_links %>%
    html_nodes("#contentContainer") %>%
    html_text() %>%
    str_trim() %>%
    toString() %>%
    str_squish() %>% 
    as_tibble()
}

df <- map_df(seq_along(html_links), scrape)

# grabbing dates (dates are from when the article was published online)
grab_dates <- function(x){
  url <- read_html(html_links[x])
  
  url %>% 
    html_nodes(".source+ .published .date") %>%
    html_text() %>% 
    as_tibble()
}


dates <- map_df(seq_along(html_links), grab_dates)
dates <- dates[-72,1]

# I want to find the impact score for each of these articles so first I need to get the doi
get_doi <- function(x){
  urls <- read_html(html_links[x])
 
  urls <- urls %>%
    html_nodes(".doi") %>%
    html_text() %>%
    str_replace("https://", "") %>%
    str_replace(".org", "") %>% 
    paste0("https://api.altmetric.com/v1/", .)
    
}


dois <- map(seq_along(html_links), get_doi)
beep(1)
dois <- unlist(dois)
ifelse(dois %in% remove_dois, 0, 1)

remove_dois <- c("https://api.altmetric.com/v1/10.1017/psrm.2014.8", "https://api.altmetric.com/v1/doi/10.1017/pan.2017.28")
test <- dois[(dois %in% remove_dois)]


# combine data and rename columns
df_1 <- cbind(df, dates, dois)
names(df) <- c("text", "date")
df <- df %>% mutate(date = dmy(date))

df$source <- "PA"

# write_csv(df, "political_analysis_text.csv")

```

```{r}
# save <- full_txt %>% 
#  select(-name) %>% 
#  rbind(political_analysis_text)
# write_csv(save, "full_text.csv")

# combine with APSJ and PSQ
tidy_txt <- full_txt %>% 
  select(-name) %>% 
  rbind(political_analysis_text) %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words)

my_stopwords <- tibble(word = c("stix", "1", "2", "3", "4", "0", "5", "x1d6fc", "e.g", "al", "6", "x_", "10", "ij", "x1d6fd", "i.e", "y_", "7", "8", "9", "10", "11", "12", "gd", "20", "tq", "13", "14","15","16","17","18","19","20", "x1d702", "x170e", "ast", "x1d707", "mathbf", "unicode"))

tidy_txt <- tidy_txt %>% 
  anti_join(my_stopwords)


```



```{r}
# topic modeling
# put into a document term matrix
txt_dtm <- tidy_txt %>% 
  select(-date) %>% 
  count(source, word, sort = T) %>% 
  cast_dtm(source, word, n)

txt_lda <- LDA(txt_dtm, k = 25, control = list(seed = 8))
beep(1)
txt_lda

# beta tells us the prob that that term would be found in that document
tidy_lda <- tidy(txt_lda)
tidy_lda %>% 
  group_by(topic, term) %>% 
  arrange(desc(beta)) %>% 
  filter(beta >= 0.001, topic == 1) %>% 
  ggplot(aes(term, beta)) +
  geom_col()
  facet_grid( . ~ topic)

tidy_lda %>% 
  group_by(topic) %>% 
  top_n(10)
```




