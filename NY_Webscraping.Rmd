---
title: "Data Sci Project"
author: "Amanda McDermott"
date: "3/29/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(jsonlite)
library(rvest)
library(purrr)
```

To gain access to the New York Times archives, you must first create a developer account and create an API key. Go to https://developer.nytimes.com/get-started for more information. Create an API key under "Archive API" which allows users to get all NYT articles for a given month. Once your app is created, copy the API key and paste into the function below. 
```{r}
api <- rstudioapi::askForSecret("Api")
```


```{r}
# create vector of dates to look at, so we're looking at NYT articles from 2000 to 2018
dates <- paste(rep(2000:2018, each = 12), rep(1:12, 19), sep = "/")
# add dates in from 2018 since there isn't a complete archive of 2018 yet
dates[229:231] <- c("2019/1", "2019/2", "2019/3")
# create urls unique to the given date and api key
url <- paste0('https://api.nytimes.com/svc/archive/v1/', dates, '.json?api-key=', api)

# functions that will be used with map
f <- c("fromJSON", "Sys.sleep")
# parameters respective to their functions
param <- list(url, 5)
# run invoke_map
ny <- invoke_map(f, param)

```
Now to make this into a workable dataframe...
```{r}
# Converting to a dataframe
# there are 7904 elements in the ny list
length(ny[[1]][["response"]][["docs"]])
list_el <- c(1:7904)

# create a function to grab elements of the list and convert them to dataframes
select_element <- function(x){
  temp <- ny[[1]][["response"]][["docs"]][[x]]
  temp <- data.frame(t(sapply(temp, c)))
}

# Make ny into a df
ny_df <- map_df(list_el, select_element)

```

Great, but now I only articles relevant to my interests. I'll filter out which articles I don't want to save time when I webscrap these later.
```{r}
unique(ny_df$section_name)
sections_interest <- c("World", "U.S.", "World; Washington", "World; Front Page", 
                       "Washington", "Front Page; U.S.", "World; Books", "World; Washington", 
                       "World; Front; Washington", "U.S.; Washington", "Washigton; Week in Review", 
                       "Front Page; U.S.; Washington", "World; Health", "Health; U.S.",
                       "Education; U.S.", "World; Education", "Technology; U.S.; Washington", 
                       "Technology; World", "Front Page; Education; U.S.; Books", 
                       "Technology; Education", "Technology; Science; Education; New York and Region",
                       "Technology; Science; Health", "Technology; Front Page; Business")

ny_urls <- ny_df %>% 
  select(-c(blog, multimedia, headline, keywords, byline, word_count, X_id, 
            slideshow_credits, subsection_name)) %>% 
  filter(section_name %in% sections_interest) %>% 
  select(web_url) # take only the urls for scraping

get_urls <- function(x){
  ny_urls$web_url[[x]]
}
elements <- c(1:917)
ny_urls <- map_chr(elements, get_urls)


```

```{r}


url <- read_html(ny_urls[1])
url %>%
  html_nodes(".story-body-text") %>% 
  html_text() %>% 
  gsub("\'", "", .) %>% 
  toString()

scrape_it <- function(x){
  url <- read_html(ny_urls[x])
  
  url %>%
  html_nodes(".story-body-text") %>% 
  html_text() %>% 
  gsub("\'", "", .) %>% 
  toString()
}

# Create a dataframe for the text and dates to go into
ny_texts <- tibble(date = NA,
                   text = c(1:917))

for(i in 1:917){
  ny_texts[i,2] <- scrape_it(i)
}
url_elements <- ny_urls[1:3]
test <- map_df(url_elements, scrape_it)



# functions that will be used with map
f <- c("scrape_it", "Sys.sleep")
# parameters respective to their functions
param <- list(url_elements, 5)
# run invoke_map
test <- invoke_map(f, param)

```





